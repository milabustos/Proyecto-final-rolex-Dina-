# -*- coding: utf-8 -*-
"""PROYECTO_DINA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NjVvZSdV3piyiaxnVN7GycnB126bX_3K

# Análisis y visualización de comentarios en redes sociales frente a la polémica de Dina Boluarte: Caso Rolex

Importar librerías y el análisis de Data
"""

# importar librerías

import pandas as pd
import nltk

#Estas líneas de recursos NLTK descargan algunos de los recursos necesarios para el procesamiento de texto:
nltk.download("punkt")
nltk.download("stopwords")
nltk.download("wordnet")

#Importamos herramientas específicas de NLTK:
from nltk import word_tokenize, FreqDist
from nltk.corpus import stopwords

#Importarmos WordCloud para crear nubes de palabras:
from wordcloud import WordCloud

#Importamos el módulo Matplotlib y la biblioteca de datos Seaborn para crear las gráficas respectivas:
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt
import seaborn as sns

# Leer data, leer los comentarios del json y crear el dataframe (df)
# df = pd.read_json("dina.json")

#Son los anteriores links donde se guardaron otros comentarios json:
# df = pd.read_json("https://firebasestorage.googleapis.com/v0/b/contador-55ade.appspot.com/o/dina.json?alt=media&token=bf6fedc1-17f8-4a73-859b-f87acce25c53")
# df = pd.read_json("https://firebasestorage.googleapis.com/v0/b/contador-55ade.appspot.com/o/dina2.json?alt=media&token=7295f476-d046-4b5d-8e59-eddcda15ad19")
# df = pd.read_json('https://firebasestorage.googleapis.com/v0/b/contador-55ade.appspot.com/o/dina.json?alt=media&token=bf6fedc1-17f8-4a73-859b-f87acce25c53')

#Utilizaremos el de 2mil comentarios para realizar el análisis:
df = pd.read_json('https://firebasestorage.googleapis.com/v0/b/contador-55ade.appspot.com/o/dina4.json?alt=media&token=758b3b66-f988-43af-b0e5-0385e7217a4f')

# Este es el ejemplo donde también se reemplaza los valores nulos con una cadena vacía:
df['text'] = df['text'].astype(str).fillna('')
df.sample(2)

#Verificamos las dimensiones de la Data:
df.shape

#Likes por comentario:
df["likesCount"].unique()

tokens_frequency = FreqDist(word_tokenize(' '.join(df["text"])))
tokens_frequency

tokens_frequency.most_common(20)

# Número de tokens:
print("Número de tokens: {}".format(tokens_frequency.N()))
# Tamaño del vocabulario o número de "types"
print("Tamaño del vocabulario: {}".format(len(tokens_frequency)))

# definir las palabras que no aportan como: la, de, los, del
stop_words = stopwords.words("spanish")
print(stop_words)

# definir función para eliminar las stop_words en español
def text_preprocess(text: str) -> str:
    tokens = [token for token in word_tokenize(text.lower()) if (token.isalpha() and (token not in stop_words))]

    return ' '.join(tokens)

#Un ejemplo de como elimina las Stop_words
print(text_preprocess("el campo con el cielo azul"))

#Creamos en data frame un campo llamado texto procesado para obtener los resultados de las palabras:

df["text_preprocessed"] = df.text.apply(text_preprocess)
df['text2'] = df['text'].str[:30]

#La frecuencia de los tokens
tokens_processed_frequency = FreqDist(word_tokenize(' '.join(df["text_preprocessed"])))

#Los más comunes
tokens_processed_frequency.most_common(20)

# Imprimimos la cantidad de palabras y el vocabulario nuevamente
print("Número de tokens: {}".format(tokens_processed_frequency.N()))

print("Tamaño del vocabulario: {}".format(len(tokens_processed_frequency)))

# Generamos el WordCloud de todas las palabras:
wordcloud = WordCloud().generate(" ".join(df.text_preprocessed))
plt.figure(figsize=[15,8])
plt.imshow(wordcloud)

#Utilizamos la librería plt para la gráfica de barras:
plt.figure(figsize=(16, 16))
df_sorted = df.sort_values('likesCount' , ascending=False) #sort_values
dfs = df_sorted.head(20)
sns.barplot(x='likesCount', y='text2', data=dfs, palette='viridis')
plt.xlabel('Número de Likes')
plt.ylabel('Comentarios')
plt.title('Número de Likes por Comentario')
plt.show()

df_sorted2 = df.sort_values(by='likesCount' , ascending=False)
df_sorted2[['likesCount' , 'text']]